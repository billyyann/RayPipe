2022-04-05 23:59:40,853 - __init__.py[line:46] - DEBUG: [ray] Forcing OMP_NUM_THREADS=1 to avoid performance degradation with many workers (issue #6998). You can override this by explicitly setting OMP_NUM_THREADS.
2022-04-05 23:59:44,323 - tpu_cluster_resolver.py[line:35] - DEBUG: Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2022-04-05 23:59:45,451 - worker.py[line:817] - DEBUG: Automatically increasing RLIMIT_NOFILE to max value of 9223372036854775807
2022-04-05 23:59:45,451 - worker.py[line:861] - INFO: Connecting to existing Ray cluster at address: 127.0.0.1:6379
2022-04-05 23:59:45,794 - log.py[line:11] - INFO: =========== Ray engine prepared =========== 
2022-04-05 23:59:45,794 - log.py[line:11] - INFO: =========== ModelProxy initialized=========== 
2022-04-05 23:59:47,766 - log.py[line:11] - INFO: =========== Trainer started =========== 
2022-04-06 00:17:32,493 - __init__.py[line:46] - DEBUG: [ray] Forcing OMP_NUM_THREADS=1 to avoid performance degradation with many workers (issue #6998). You can override this by explicitly setting OMP_NUM_THREADS.
2022-04-06 00:17:36,454 - tpu_cluster_resolver.py[line:35] - DEBUG: Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2022-04-06 00:17:37,711 - worker.py[line:817] - DEBUG: Automatically increasing RLIMIT_NOFILE to max value of 9223372036854775807
2022-04-06 00:17:37,711 - worker.py[line:861] - INFO: Connecting to existing Ray cluster at address: 127.0.0.1:6379
2022-04-06 00:17:38,072 - log.py[line:11] - INFO: =========== Ray engine prepared =========== 
2022-04-06 00:17:38,072 - log.py[line:11] - INFO: =========== ModelProxy initialized=========== 
2022-04-06 00:17:40,333 - log.py[line:11] - INFO: =========== Trainer started =========== 
2022-04-06 00:21:44,742 - __init__.py[line:46] - DEBUG: [ray] Forcing OMP_NUM_THREADS=1 to avoid performance degradation with many workers (issue #6998). You can override this by explicitly setting OMP_NUM_THREADS.
2022-04-06 00:22:12,774 - __init__.py[line:46] - DEBUG: [ray] Forcing OMP_NUM_THREADS=1 to avoid performance degradation with many workers (issue #6998). You can override this by explicitly setting OMP_NUM_THREADS.
2022-04-06 00:22:16,378 - tpu_cluster_resolver.py[line:35] - DEBUG: Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2022-04-06 00:22:17,517 - worker.py[line:817] - DEBUG: Automatically increasing RLIMIT_NOFILE to max value of 9223372036854775807
2022-04-06 00:22:17,517 - worker.py[line:861] - INFO: Connecting to existing Ray cluster at address: 127.0.0.1:6379
2022-04-06 00:22:17,865 - log.py[line:11] - INFO: =========== Ray engine prepared =========== 
2022-04-06 00:22:17,865 - log.py[line:11] - INFO: =========== ModelProxy initialized=========== 
2022-04-06 00:22:19,938 - log.py[line:11] - INFO: =========== Trainer started =========== 
